{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_page(url, retries=2):\n",
    "    \"\"\"Funkcja pobierająca zawartość strony z obsługą ponownych prób.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Warning: Unable to fetch page {url}, status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Exception occurred while fetching page {url}: {e}\")\n",
    "        time.sleep(1)  # Opóźnienie między próbami\n",
    "    return None\n",
    "\n",
    "def alphanumeric_to_numeric_id(alphanumeric_id):\n",
    "    hash_object = hashlib.sha256(alphanumeric_id.encode())\n",
    "    numeric_id = int(hash_object.hexdigest(), 16)\n",
    "    return str(numeric_id)[:15]\n",
    "\n",
    "def scrape_offers(base_url, max_pages=100):\n",
    "    \"\"\"Funkcja pobierająca wszystkie ogłoszenia z podanej liczby stron.\"\"\"\n",
    "    all_items = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        html_content = fetch_page(url)\n",
    "        if not html_content:\n",
    "            print(f\"Skipping page {page} due to errors.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        items = soup.find_all(\"article\", class_=re.compile(r\"ooa-1yux8sr\"))\n",
    "\n",
    "        if not items:\n",
    "            print(f\"No items found on page {page}.\")\n",
    "            continue\n",
    "\n",
    "        all_items.extend(items)\n",
    "\n",
    "    return all_items\n",
    "\n",
    "def parse_offers(items):\n",
    "    \"\"\"Funkcja wyodrębniająca tytuły, adresy URL i ID z ogłoszeń.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for offer in items:\n",
    "        try:\n",
    "            title_url_container = offer.find(\"h2\", class_=re.compile(r\"ooa-1kyyooz\"))\n",
    "            if not title_url_container:\n",
    "                print(\"Warning: Title container not found.\")\n",
    "                continue\n",
    "\n",
    "            title = title_url_container.find(\"a\")\n",
    "            if not title or not title.get(\"href\"):\n",
    "                print(\"Warning: Title or URL not found.\")\n",
    "                continue\n",
    "\n",
    "            url = title[\"href\"]\n",
    "            id_numeric = alphanumeric_to_numeric_id(url.split('-')[-1].split('.')[0])\n",
    "\n",
    "            results.append({\"url\": url, \"id\": id_numeric})\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to parse an offer: {e}\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Fetching page 25...\n",
      "Fetching page 26...\n",
      "Fetching page 27...\n",
      "Fetching page 28...\n",
      "Fetching page 29...\n",
      "Fetching page 30...\n",
      "Fetching page 31...\n",
      "Fetching page 32...\n",
      "Fetching page 33...\n",
      "Fetching page 34...\n",
      "Fetching page 35...\n",
      "Fetching page 36...\n",
      "Fetching page 37...\n",
      "Fetching page 38...\n",
      "Fetching page 39...\n",
      "Fetching page 40...\n",
      "Fetching page 41...\n",
      "Fetching page 42...\n",
      "Fetching page 43...\n",
      "Fetching page 44...\n",
      "Fetching page 45...\n",
      "Fetching page 46...\n",
      "Fetching page 47...\n",
      "Fetching page 48...\n",
      "Fetching page 49...\n",
      "Fetching page 50...\n",
      "Fetching page 51...\n",
      "Fetching page 52...\n",
      "Fetching page 53...\n",
      "Fetching page 54...\n",
      "Fetching page 55...\n",
      "Fetching page 56...\n",
      "Fetching page 57...\n",
      "Fetching page 58...\n",
      "Fetching page 59...\n",
      "Fetching page 60...\n",
      "Fetching page 61...\n",
      "Fetching page 62...\n",
      "Fetching page 63...\n",
      "Fetching page 64...\n",
      "Fetching page 65...\n",
      "Fetching page 66...\n",
      "Fetching page 67...\n",
      "Fetching page 68...\n",
      "Fetching page 69...\n",
      "Fetching page 70...\n",
      "Fetching page 71...\n",
      "Fetching page 72...\n",
      "Fetching page 73...\n",
      "Fetching page 74...\n",
      "Fetching page 75...\n",
      "Fetching page 76...\n",
      "Fetching page 77...\n",
      "Fetching page 78...\n",
      "Fetching page 79...\n",
      "Fetching page 80...\n",
      "Fetching page 81...\n",
      "Fetching page 82...\n",
      "Fetching page 83...\n",
      "Fetching page 84...\n",
      "Fetching page 85...\n",
      "Fetching page 86...\n",
      "Fetching page 87...\n",
      "Fetching page 88...\n",
      "Fetching page 89...\n",
      "Fetching page 90...\n",
      "Fetching page 91...\n",
      "Fetching page 92...\n",
      "Fetching page 93...\n",
      "Fetching page 94...\n",
      "Fetching page 95...\n",
      "Fetching page 96...\n",
      "Fetching page 97...\n",
      "Fetching page 98...\n",
      "Fetching page 99...\n",
      "Fetching page 100...\n",
      "Fetching page 101...\n",
      "Fetching page 102...\n",
      "Fetching page 103...\n",
      "Fetching page 104...\n",
      "Fetching page 105...\n",
      "Fetching page 106...\n",
      "Fetching page 107...\n",
      "Fetching page 108...\n",
      "Fetching page 109...\n",
      "Fetching page 110...\n",
      "Fetching page 111...\n",
      "Fetching page 112...\n",
      "Fetching page 113...\n",
      "Fetching page 114...\n",
      "Fetching page 115...\n",
      "Fetching page 116...\n",
      "Fetching page 117...\n",
      "Fetching page 118...\n",
      "Fetching page 119...\n",
      "Fetching page 120...\n",
      "Zapisano 3840 ogłoszeń do pliku '../data/all_urls.json'.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.otomoto.pl/osobowe/bmw\"\n",
    "max_pages = 120 # Ustaw limit maksymalnej liczby stron do przeszukania\n",
    "\n",
    "# Pobranie wszystkich ofert\n",
    "all_items_raw = scrape_offers(base_url, max_pages=max_pages)\n",
    "\n",
    "# Przetwarzanie ofert\n",
    "all_items_parsed = parse_offers(all_items_raw)\n",
    "\n",
    "# Zapis do pliku JSON Lines\n",
    "output_path = Path(\"../data/all_urls.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)  # Tworzy katalog, jeśli nie istnieje\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(all_items_parsed, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Zapisano {len(all_items_parsed)} ogłoszeń do pliku '{output_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
