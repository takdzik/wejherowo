{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_page(url, retries=2):\n",
    "    \"\"\"Funkcja pobierająca zawartość strony z obsługą ponownych prób.\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Warning: Unable to fetch page {url}, status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Exception occurred while fetching page {url}: {e}\")\n",
    "        time.sleep(1)  # Opóźnienie między próbami\n",
    "    return None\n",
    "\n",
    "def alphanumeric_to_numeric_id(alphanumeric_id):\n",
    "    hash_object = hashlib.sha256(alphanumeric_id.encode())\n",
    "    numeric_id = int(hash_object.hexdigest(), 16)\n",
    "    return str(numeric_id)[:15]\n",
    "\n",
    "def scrape_offers(base_url, max_pages=100):\n",
    "    \"\"\"Funkcja pobierająca wszystkie ogłoszenia z podanej liczby stron.\"\"\"\n",
    "    all_items = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        html_content = fetch_page(url)\n",
    "        if not html_content:\n",
    "            print(f\"Skipping page {page} due to errors.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        items = soup.find_all(\"article\", class_=re.compile(r\"ooa-1yux8sr\"))\n",
    "\n",
    "        if not items:\n",
    "            print(f\"No items found on page {page}.\")\n",
    "            continue\n",
    "\n",
    "        all_items.extend(items)\n",
    "\n",
    "    return all_items\n",
    "\n",
    "def parse_offers(items):\n",
    "    \"\"\"Funkcja wyodrębniająca tytuły, adresy URL i ID z ogłoszeń.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for offer in items:\n",
    "        try:\n",
    "            title_url_container = offer.find(\"h2\", class_=re.compile(r\"ooa-1kyyooz\"))\n",
    "            if not title_url_container:\n",
    "                print(\"Warning: Title container not found.\")\n",
    "                continue\n",
    "\n",
    "            title = title_url_container.find(\"a\")\n",
    "            if not title or not title.get(\"href\"):\n",
    "                print(\"Warning: Title or URL not found.\")\n",
    "                continue\n",
    "\n",
    "            url = title[\"href\"]\n",
    "            id_numeric = alphanumeric_to_numeric_id(url.split('-')[-1].split('.')[0])\n",
    "\n",
    "            results.append({\"url\": url, \"id\": id_numeric})\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to parse an offer: {e}\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Zapisano 640 ogłoszeń do pliku '..\\data\\all_urls.json'.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.otomoto.pl/osobowe/bmw\"\n",
    "max_pages = 20 # Ustaw limit maksymalnej liczby stron do przeszukania\n",
    "\n",
    "# Pobranie wszystkich ofert\n",
    "all_items_raw = scrape_offers(base_url, max_pages=max_pages)\n",
    "\n",
    "# Przetwarzanie ofert\n",
    "all_items_parsed = parse_offers(all_items_raw)\n",
    "\n",
    "# Zapis do pliku JSON Lines\n",
    "output_path = Path(\"../data/all_urls.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)  # Tworzy katalog, jeśli nie istnieje\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(all_items_parsed, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(f\"Zapisano {len(all_items_parsed)} ogłoszeń do pliku '{output_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
