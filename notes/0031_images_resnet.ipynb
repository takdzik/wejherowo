{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import time\n",
    "\n",
    "# Parametry\n",
    "JSON_PATH = \"../data/clean_data_train_01.json\"\n",
    "IMAGE_DIR = \"../data/images/\"\n",
    "MODEL_SAVE_DIR = \"../models/\"\n",
    "IMG_HEIGHT, IMG_WIDTH = 140, 100\n",
    "NUM_IMAGES = 12\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 55\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, transform=None, num_images=NUM_IMAGES, debug=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.num_images = num_images\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wczytaj dane JSON\n",
    "        self.data = pd.read_json(json_path, lines=True)\n",
    "\n",
    "        # Filtruj dane, aby uwzględnić tylko oferty z przynajmniej jednym zdjęciem\n",
    "        self.samples = []\n",
    "        for _, row in self.data.iterrows():\n",
    "            car_id = str(row[\"id\"])\n",
    "            has_images = any(\n",
    "                Path(self.image_dir, f\"{car_id}_{i}.jpg\").exists() for i in range(1, self.num_images + 1)\n",
    "            )\n",
    "            if has_images:\n",
    "                self.samples.append({\n",
    "                    \"id\": car_id,\n",
    "                    \"price\": row[\"cena\"],  # Cena samochodu\n",
    "                })\n",
    "\n",
    "        print(f\"Debug: Dataset contains {len(self.samples)} samples.\")\n",
    "\n",
    "        # Wczytaj wszystkie dane jednorazowo do pamięci RAM\n",
    "        self.loaded_data = []\n",
    "        for idx, sample in enumerate(self.samples):\n",
    "            car_id = sample[\"id\"]\n",
    "            price = sample[\"price\"]\n",
    "\n",
    "            # Załaduj zdjęcia\n",
    "            images = []\n",
    "            for i in range(1, self.num_images + 1):\n",
    "                image_path = Path(self.image_dir, f\"{car_id}_{i}.jpg\")\n",
    "                if image_path.exists():\n",
    "                    img = Image.open(image_path).convert(\"RGB\")\n",
    "                    if self.transform:\n",
    "                        img = self.transform(img)\n",
    "                    images.append(img)\n",
    "\n",
    "            # Jeśli brakuje zdjęć, powielaj istniejące\n",
    "            while len(images) < self.num_images:\n",
    "                images.append(images[-1])\n",
    "\n",
    "            # Przytnij, jeśli jest za dużo zdjęć\n",
    "            images = images[:self.num_images]\n",
    "            images = torch.stack(images)  # [NUM_IMAGES, C, H, W]\n",
    "\n",
    "            # Zapisz przetworzone dane w pamięci\n",
    "            self.loaded_data.append((images, torch.tensor(price, dtype=torch.float32)))\n",
    "\n",
    "            # Loguj co 100 próbek\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Loaded {idx + 1}/{len(self.samples)} samples into memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loaded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pobierz dane bezpośrednio z pamięci\n",
    "        return self.loaded_data[idx]\n",
    "\n",
    "\n",
    "# Model\n",
    "class CarPriceRegressor(nn.Module):\n",
    "    def __init__(self, num_images=NUM_IMAGES):\n",
    "        super(CarPriceRegressor, self).__init__()\n",
    "        # Model bazowy (ResNet18)\n",
    "        self.base_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.base_model.fc = nn.Identity()  # Usuń klasyfikator na końcu\n",
    "\n",
    "        # Przetwarzanie obrazów\n",
    "        self.embedding_size = 512  # Rozmiar embeddingu z ResNet18\n",
    "        self.fc = nn.Linear(self.embedding_size, 1)  # Regresja zamiast klasyfikacji\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [BATCH_SIZE, NUM_IMAGES, C, H, W]\n",
    "        batch_size, num_images, C, H, W = x.size()\n",
    "        \n",
    "        # Przetwarzanie każdego obrazu w ramach jednej oferty\n",
    "        embeddings = []\n",
    "        for i in range(num_images):\n",
    "            img_features = self.base_model(x[:, i, :, :, :])  # Przetwarzanie pojedynczego obrazu\n",
    "            embeddings.append(img_features)\n",
    "        \n",
    "        # Łączenie cech obrazów przy użyciu max pooling\n",
    "        embeddings = torch.stack(embeddings, dim=1)  # [BATCH_SIZE, NUM_IMAGES, EMBEDDING_SIZE]\n",
    "        combined_embedding, _ = embeddings.max(dim=1)  # [BATCH_SIZE, EMBEDDING_SIZE]\n",
    "        \n",
    "        # Regresja na podstawie połączonych cech\n",
    "        output = self.fc(combined_embedding)  # [BATCH_SIZE, 1]\n",
    "        return output.squeeze()  # [BATCH_SIZE]\n",
    "\n",
    "# Transformacje dla obrazów\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset i DataLoader\n",
    "#MAX_SAMPLES = 5000\n",
    "train_dataset = CarDataset(JSON_PATH, IMAGE_DIR, transform=transform) \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "class MAPE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPE, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        epsilon = 1e-6  # Aby uniknąć dzielenia przez zero\n",
    "        mape = torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 100\n",
    "        return mape\n",
    "\n",
    "\n",
    "# Model, funkcja kosztu, optymalizator\n",
    "model = CarPriceRegressor(num_images=NUM_IMAGES).to(DEVICE)\n",
    "criterion = MAPE()  # Funkcja kosztu dla regresji\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trenowanie\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Rozpocznij pomiar czasu dla całej epoki\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_mape = 0\n",
    "\n",
    "    # Inicjalizacja liczników czasu\n",
    "    data_loading_time = 0\n",
    "    computation_time = 0\n",
    "    data_transfer_time = 0\n",
    "    x_time = time.time()\n",
    "    \n",
    "    for i, (images, prices) in enumerate(train_loader, start=1):\n",
    "        # Pomiar czasu na początku iteracji\n",
    "        iter_start_time = time.time()\n",
    "\n",
    "        # Rozpocznij pomiar czasu ładowania danych\n",
    "        start_time = time.time()\n",
    "        # Przenieś dane na urządzenie\n",
    "        images, prices = images.to(DEVICE), prices.to(DEVICE)\n",
    "        load_time = time.time() - start_time\n",
    "        data_loading_time += load_time\n",
    "\n",
    "        # Rozpocznij pomiar czasu transferu danych\n",
    "        start_time = time.time()\n",
    "        transfer_time = time.time() - start_time\n",
    "        data_transfer_time += transfer_time\n",
    "\n",
    "        # Rozpocznij pomiar czasu obliczeń\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, prices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        comp_time = time.time() - start_time\n",
    "        computation_time += comp_time\n",
    "\n",
    "        # Akumulacja metryk\n",
    "        epoch_loss += loss.item()\n",
    "        total_mape += torch.mean(torch.abs((prices - outputs) / prices)).item() * 100\n",
    "\n",
    "        # Koniec pomiaru czasu iteracji\n",
    "        iter_total_time = time.time() - iter_start_time\n",
    "        x_total_time = time.time() - x_time\n",
    "        x_time = time.time()\n",
    "\n",
    "        # Logowanie co 10 iteracji\n",
    "        # if i % 100 == 0:\n",
    "        #     print(\n",
    "        #         f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Iteration [{i}/{len(train_loader)}]: \"\n",
    "        #         f\"Total Time: {iter_total_time:.4f}s, \"\n",
    "        #         f\"Data Loading Time: {load_time:.4f}s, \"\n",
    "        #         f\"Computation Time: {comp_time:.4f}s, \"\n",
    "        #         f\"Data Transfer Time: {transfer_time:.4f}s\"\n",
    "        #         f\"X Time: {x_total_time:.4f}s, \"\n",
    "        #     )\n",
    "\n",
    "    # Oblicz czas trwania epoki\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    # Średnia strata i MAPE\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_mape = total_mape / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Summary:\")\n",
    "    print(f\"  Train Loss: {avg_loss:.4f}, Train MAPE: {avg_mape:.2f}%\")\n",
    "    print(f\"  Total Data Loading Time: {data_loading_time:.2f}s, Total Computation Time: {computation_time:.2f}s, Total Data Transfer Time: {data_transfer_time:.2f}s\")\n",
    "    print(f\"  Total Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "\n",
    "# Tworzenie katalogu na zapis modelu\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Zapis modelu\n",
    "model_path = os.path.join(MODEL_SAVE_DIR, \"car_price_regressor.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model zapisany w: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path, device=\"cuda\"):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()  # Przełącz model w tryb ewaluacji\n",
    "    print(f\"Model loaded from {model_path}.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device=\"cuda\", log_interval=100):\n",
    "    print(\"Starting evaluation...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mape = 0\n",
    "    total_samples = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, prices) in enumerate(test_loader):\n",
    "            images, prices = images.to(device), prices.to(device)\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Akumulacja strat\n",
    "            loss = criterion(predictions, prices)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_mape += torch.sum(torch.abs((prices - predictions) / (prices + 1e-6))).item() * 100\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "            # Zbieranie przewidywań i wartości rzeczywistych\n",
    "            y_pred.extend(predictions.cpu().tolist())\n",
    "            y_true.extend(prices.cpu().tolist())\n",
    "\n",
    "            # Logowanie częściowych wyników\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                print(f\"Batch {batch_idx + 1}/{len(test_loader)}:\")\n",
    "                print(f\"  Partial Loss: {loss.item():.4f}, Partial MAPE: {torch.mean(torch.abs((prices - predictions) / (prices + 1e-6))).item() * 100:.2f}%\")\n",
    "    \n",
    "    # Obliczanie średnich metryk\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_mape = total_mape / total_samples\n",
    "\n",
    "    print(f\"Evaluation Summary: MAPE: {avg_mape:.2f}%, Average Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss, avg_mape, y_pred, y_true\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
